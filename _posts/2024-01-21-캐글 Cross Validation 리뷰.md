---
layout: post
title: "캐글 Cross Validation 리뷰"
date: 2024-01-21 19:15:00 
categories: Python DataScience Kaggle ML
---

머신 러닝을 주로 학습세트(Training set)와 평가세트(Validation Set)를 나누고 학습세트를 통해 학습한 모델을 평가세트로 검증하므로 모델의 정확도를 측정한다. 평가세트를 얼마나 크게, 또는 작게 나눌지는 모델을 실행시키는 사람의 마음이지만, 원천 데이터는 쪼개서 두 가지 용도로 사용한다는 것은 변함없다. 주로 학습세트와 평가세트를 8:2로 나누고, 파이썬은 `sklearn` 모듈의 `model_selection.train_test_split` 메서드로 아래와 같이 쉽게 나눌 수 있다.

```python
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=1)
```

> 위 코드에서는 `X_train`과 `y_train`을 각각 학습세트와 평가세트로 8:2비율(test_size=0.2)로 나눈다. `shuffle`은 데이터를 나누기 전에 섞을 것을 의미하고, `random_state`는 이후에 해당 코드를 다시 실행해도 동일한 결과가 나올것을 보장한다.

이런식으로 원천 데이터를 나누는 것은 모델에 대해 데이터 주도적 의사결정을 나눌 수 있게 해주지만, 무작위성이라는 단점이 존재한다. 데이터를 나눌 때 임의로 나뉘기 때문에 학습된 모델이 특정 평가세트에서 높은 점수를 받았다고 해서 실제로 사용되었을 때 정확할 것이라고는 확정지을 수 없다는 것이다. 다시 말하면, 무작위로 특정 평가세트에만 잘 맞는 모델이 생성되었을 수도 있다는 말이다. 이 글은 이런 문제를 다루기 위한 방법인 교차 검증(Cross Validation)을 다루고자 한다.

# 교차 검증(Cross Validation)이란?
> 이 글에서 다루는 교차 검증은 K-Fold에 해당된다. Stratified K-Fold는 나중에 배우면 설명하겠다.
교차 검증은 말 그대로 평가세트를 돌려가며 모델의 정확도를 검증하는 것이다. 예를 들어, 원천 데이터를 학습과 평가세트로, 8:2로 나누려고 할 경우, 각기 다른 평가세트를 총 5판 만들 수 있다는 것이다. 이렇게 나뉜 평가세트 한 판을 Fold라고 부른다. 교차 검증은 이렇게 원천 데이터를 여러 판으로 나눠 원천 데이터 모두를 평가세트로 활용하는 것을 의미한다.

교차 검증을 진행할 경우 모든 데이터를 평가세트로 활용하기 때문에 모델 정확도에 대해 더욱 올바른 평가 지표를 제공한다. 모델 정확도를 더 잘 나타낼 수 있다는 것은, 모델 관련 의사결정을 내릴 때 객관적인 수치를 제공하므로써 쉽게 만들어줄 수 있다. 하지만 교차 검증이 무조건 좋은 것 만은 아니다.

위에서 말했던 것 처럼 교차 검증을 위해서는 평가세트를 여러 판 만들고, 각 평가세트마다 각기 다른 학습세트로 학습을 진행하고 평가해야 한다. 그러므로 자연스럽게 시간이 더 오래 걸릴 수 밖에 없다. 그리고, 원천 데이터가 이미 충분히 크다면 굳이 교차 검증을 할 필요도 없다. 교차 검증의 목표는 데이터 일부분에 과잉 학습되는 것을 방지하기 위해 진행하는 것인데, 원천 데이터가 크다면 그럴 확률은 매우 낮다. 

여기서 당연히 크다라는 기준이 궁금할텐데, 캐글에서는 몇 분내로 모델이 돌아간다면 그냥 교차 검증해도 될 것이라고 말한다.

## 활용 예시
이미 작성해둔 파이프라인이 있다고 가정해보자.
```python
from sklearn.pipeline import Pipeline

my_pipeline = Pipeline(steps) #steps 는 파이프라인에서 실행할 일련의 과정을 나타낸다
```
이 파이프라인은 이미 학습세트와 평가세트를 나눈 상태로 돌아가고 있지만, `sklearn`의 `model_selection.cross_val_score`를 통해서 간단하게 교차 검증을 실행할 수 있다.

```python
from sklearn.model_selection import cross_val_score

scores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring="neg_mean_absolute_error") # 여기서는 MAE값을 음수로 받도록 설정했으므로 앞에 -1을 곱해준다.
```
`cross_val_score` 메서드는 파이프라인과 원천 데이터, 몇 판을 만들 것인지를 의미하는 `cv`, 점수를 어떻게 받고 싶은지를 설정할 수 있는 `scoring`을 인자로 받는다. 여기서 점수를 어떻게 매길지에 대해 더 자세한 정보는 [Scikit-learn의 공식문서](https://scikit-learn.org/stable/modules/model_evaluation.html)에서 확인할 수 있다.

우리는 위에서 `cv`를 5로 설정했기 때문에 교차 검증을 총 5판을 대상으로 진행하며, 5개의 점수를 받는다. 모델 정확성에 대해 더 현실적인 지표를 얻기 위해서는 `cross_val_score`에서 반환된 5개의 값의 평균을 내주면 된다.

# 결론
내가 지금까지 다룬 머신 러닝은 모델 학습은 이미 구축된 코드를 통해 쉽게 생성해줄 수 있다. 하지만 그렇게 생성된 모델의 정확도를 높이기 위해 다양한 변수를 조금씩 바꿔보며 실험해보는 시간이 많았다. 이렇게 여러 실험을 거쳐 생성된 모델이 정확도 지표가 현실적이지 않아서 실제 실행했을 때 기대했던 만큼 결과가 나오지 않는다면 얼마나 억울할까? 교차 검증같은 정확도를 더 정확하게 측정하는 지표는 개발자가 적어도 억울할 일은 없게끔 해주는 것 같다.


